{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17YgzS_F7oeBo1wsVMuwkdNxV87emUVp4",
      "authorship_tag": "ABX9TyP4RZWxLSAurwP1cMb/lQc/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cad4f3d85b6b41f587c2e25ae3f876f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4042b39b5574ff8bf813942fe747703",
              "IPY_MODEL_5d184e3a850b470f9a4e7b34747c7804"
            ],
            "layout": "IPY_MODEL_f8c507c116f14c279b89a6bd5118fb77"
          }
        },
        "f4042b39b5574ff8bf813942fe747703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40c73dc246784395a9461c1792864ff4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9b8a25c42ea6485fa293f3e9765d0024",
            "value": "0.001 MB of 0.009 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "5d184e3a850b470f9a4e7b34747c7804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e4457a2e93a48eba9b65ceeaee0a245",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_855d2306bd9d4b13b41e5ccad258ae49",
            "value": 0.1055881121808288
          }
        },
        "f8c507c116f14c279b89a6bd5118fb77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40c73dc246784395a9461c1792864ff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b8a25c42ea6485fa293f3e9765d0024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e4457a2e93a48eba9b65ceeaee0a245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "855d2306bd9d4b13b41e5ccad258ae49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fred-dev/wav_gan/blob/main/Fred_WAV_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  audio_folder = \"/content/drive/MyDrive/colab_storage/ronxgin_data_samples\"\n",
        "  json_folder = \"/content/drive/MyDrive/colab_storage/ronxgin_data_samples\"\n",
        "  model_path = \"/content/drive/MyDrive/colab_storage/colab_output\"\n",
        "  output_path = \"/content/drive/MyDrive/colab_storage/colab_output/\"\n",
        "\n"
      ],
      "metadata": {
        "id": "j-bx2eTsawdQ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n"
      ],
      "metadata": {
        "id": "sNqJUbmNZn5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I2fHyssYY8N6"
      },
      "outputs": [],
      "source": [
        "# 1. Import required libraries\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio.transforms as T\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_sequence\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define the dataset class\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, audio_folder, json_folder, transform=None):\n",
        "        self.audio_folder = audio_folder\n",
        "        self.json_folder = json_folder\n",
        "        self.transform = transform\n",
        "        self.MAX_LENGTH = 400\n",
        "        self.MAX_NUM_FRAMES = 5 * 44100  # 5 seconds of audio frames\n",
        "        self.mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=44100, n_mels=30)\n",
        "        self.file_list = sorted([f for f in os.listdir(audio_folder) if f.endswith(('.wav'))]) # Update this line to only include audio files\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = os.path.join(self.audio_folder, self.file_list[idx])\n",
        "        json_path = os.path.join(self.json_folder, os.path.splitext(self.file_list[idx])[0].rstrip('_P') + \".json\")\n",
        "\n",
        "        print(f\"Loading {idx+1}/{len(self.file_list)}: {audio_path}\")\n",
        "\n",
        "        waveform, _ = torchaudio.load(audio_path, num_frames=self.MAX_NUM_FRAMES)\n",
        "\n",
        "        with open(json_path) as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        params = [\n",
        "            data[\"coord\"][\"lat\"],\n",
        "            data[\"coord\"][\"lon\"],\n",
        "            data[\"wind\"][\"deg\"],\n",
        "            data[\"main\"][\"humidity\"],\n",
        "            data[\"wind\"][\"speed\"],\n",
        "            data[\"wind\"][\"deg\"],\n",
        "            data[\"main\"][\"pressure\"],\n",
        "            data[\"elevation\"],\n",
        "            data[\"minutesOfDay\"],\n",
        "            data[\"dayOfYear\"],\n",
        "        ]\n",
        "\n",
        "        mel_spec = self.mel_transform(waveform.squeeze())\n",
        "        mel_spec = mel_spec[:, :self.MAX_LENGTH]\n",
        "\n",
        "        params_tensor = torch.tensor(params, dtype=torch.float32).unsqueeze(1)\n",
        "        params_tensor = params_tensor.expand(-1, mel_spec.size(1))\n",
        "\n",
        "        features = torch.cat((mel_spec, params_tensor), dim=0)\n",
        "\n",
        "        return features, torch.tensor(idx, dtype=torch.int64)\n",
        "\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Sort the batch by sequence length (descending order)\n",
        "    batch = sorted(batch, key=lambda x: x[0].size(1), reverse=True)\n",
        "\n",
        "    # Create a list of the sequence lengths for packed sequences\n",
        "    seq_lengths = [x[0].size(1) for x in batch]\n",
        "\n",
        "    # Pad the batch to have sequences of equal length\n",
        "    padded_waveforms = torch.zeros(len(batch), batch[0][0].size(0), max(seq_lengths))\n",
        "    for i, (waveform, _, _) in enumerate(batch):\n",
        "        padded_waveforms[i, :, :seq_lengths[i]] = waveform\n",
        "\n",
        "    # Convert the padded batch to a packed sequence\n",
        "    packed_batch = nn.utils.rnn.pack_sequence([x for x in padded_waveforms], enforce_sorted=False)\n",
        "\n",
        "    params_list = [x[1] for x in batch]\n",
        "    params_tensor = torch.stack(params_list, dim=0)\n",
        "\n",
        "    return packed_batch, params_tensor, [x[2] for x in batch]\n",
        "\n",
        "\n",
        "\n",
        "# Define the Generator class\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, audio_dim):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.audio_dim = audio_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, audio_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, _ = self.lstm(x)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "# Define the Discriminator class\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, audio_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.audio_dim = audio_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(audio_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, params):\n",
        "        x_with_params = torch.cat((x, params.unsqueeze(1).repeat(1, x.size(1), 1)), dim=2)\n",
        "        output, _ = self.lstm(x_with_params)\n",
        "        output = self.linear(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "def train_discriminator(real_data, fake_data, params, optimizer, criterion):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    real_preds = discriminator(real_data, params)\n",
        "\n",
        "    real_loss = criterion(real_preds, torch.ones_like(real_preds))\n",
        "\n",
        "    fake_preds = discriminator(fake_data, params)\n",
        "\n",
        "    fake_loss = criterion(fake_preds, torch.zeros_like(fake_preds))\n",
        "\n",
        "    total_loss = real_loss + fake_loss\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return total_loss.item()\n",
        "\n",
        "def train_generator(fake_data, params, optimizer, criterion):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    preds = discriminator(fake_data, params)\n",
        "\n",
        "    loss = criterion(preds, torch.ones_like(preds))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def train_gan(audio_folder, json_folder, epochs, batch_size, learning_rate, device, save_interval, model_path):\n",
        "    MAX_LENGTH = 431  # You can adjust this value based on your requirements\n",
        "\n",
        "    # Define the MEL spectrogram transformation\n",
        "    mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(sample_rate=44100, n_mels=128, hop_length=1024, n_fft=2048)\n",
        "    mel_spectrogram_transform.n_mels = 30 # Update the number of Mel bands\n",
        "\n",
        "    dataset = AudioDataset(audio_folder, json_folder, transform=mel_spectrogram_transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "    input_dim = 40 # Update the input dimension\n",
        "    hidden_dim = 128\n",
        "    num_layers = 1\n",
        "    audio_dim = 30  # Assuming the Mel spectrogram has 30 dimensions. Update the audio dimension\n",
        "    generator = Generator(input_dim, hidden_dim, num_layers, audio_dim).to(device)\n",
        "\n",
        "    discriminator = Discriminator(audio_dim, hidden_dim, num_layers, 1).to(device)\n",
        "\n",
        "    initial_generator_path = os.path.join(model_path, \"generator_initial.pth\")\n",
        "    initial_discriminator_path = os.path.join(model_path, \"discriminator_initial.pth\")\n",
        "\n",
        "    if not os.path.exists(initial_generator_path):\n",
        "        torch.save(generator.state_dict(), initial_generator_path)\n",
        "\n",
        "    if not os.path.exists(initial_discriminator_path):\n",
        "        torch.save(discriminator.state_dict(), initial_discriminator_path)\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
        "    optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        for batch_idx, (packed_real_data, params, labels) in enumerate(dataloader):\n",
        "            packed_real_data, params = packed_real_data.to(device), params.to(device)\n",
        "            batch_size = packed_real_data.batch_sizes[0]\n",
        "\n",
        "\n",
        "            # Train discriminator\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            noise = torch.randn(batch_size, input_dim, device=device)\n",
        "            z = torch.cat((noise.unsqueeze(1), params.unsqueeze(1)), dim=2)\n",
        "            packed_fake_data = generator(z)\n",
        "\n",
        "            real_data, _ = pad_packed_sequence(packed_real_data, batch_first=True)\n",
        "            fake_data, _ = pad_packed_sequence(packed_fake_data, batch_first=True)\n",
        "\n",
        "            real_validity = discriminator(torch.cat((real_data, params.unsqueeze(1).repeat(1, real_data.size(1), 1)), dim=2))\n",
        "\n",
        "            fake_validity = discriminator(torch.cat((fake_data.detach(), params.unsqueeze(1).repeat(1, fake_data.size(1), 1)), dim=2))\n",
        "\n",
        "            real_loss = criterion(real_validity, torch.ones(batch_size, 1, device=device))\n",
        "            fake_loss = criterion(fake_validity, torch.zeros(batch_size, 1, device=device))\n",
        "            d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # Train generator\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            fake_validity = discriminator(torch.cat((fake_data, params.unsqueeze(1).repeat(1, fake_data.size(1), 1)), dim=2))\n",
        "            g_loss = criterion(fake_validity, torch.ones(batch_size, 1, device=device))\n",
        "\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            print(f\"Epoch [{epoch}/{epochs}] Batch [{batch_idx+1}/{len(dataloader)}] Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}\")\n",
        "\n",
        "        if epoch % save_interval == 0:\n",
        "            torch.save(generator.state_dict(), os.path.join(model_path, f\"generator_epoch_{epoch}.pth\"))\n",
        "            torch.save(discriminator.state_dict(), os.path.join(model_path, f\"discriminator_epoch_{epoch}.pth\"))\n",
        "\n",
        "    torch.save(generator.state_dict(), os.path.join(model_path, \"generator_final.pth\"))\n",
        "    torch.save(discriminator.state_dict(), os.path.join(model_path, \"discriminator_final.pth\"))\n",
        "\n",
        "def generate_audio(generator_path, params, duration, output_folder, device):\n",
        "    generator = Generator(input_dim=10 + 128 * 128, output_dim=128 * 128, hidden_size=256, num_layers=2).to(device)\n",
        "    generator.load_state_dict(torch.load(generator_path))\n",
        "    generator.eval()\n",
        "\n",
        "    params = torch.tensor(params, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    num_steps = int(duration * 44100 / 1024)\n",
        "    generated_waveforms = []\n",
        "\n",
        "    print(\"Generating audio...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = None\n",
        "        for step in range(num_steps):\n",
        "            if hidden is None:\n",
        "                noise = torch.randn(1, 1, 128 * 128 - 10, device=device)\n",
        "            else:\n",
        "                noise = torch.randn(1, 1, 128 * 128 - 10, device=device)\n",
        "\n",
        "            z = torch.cat((noise, params.unsqueeze(1)), dim=2)\n",
        "            output, hidden = generator(z, hidden)\n",
        "            output_waveform = output.squeeze().detach().cpu()\n",
        "            generated_waveforms.append(output_waveform)\n",
        "\n",
        "            if step % (num_steps // 10) == 0:\n",
        "                print(f\"Step {step}/{num_steps}\")\n",
        "\n",
        "    print(\"Audio generation completed.\")\n",
        "\n",
        "    generated_waveform = torch.cat(generated_waveforms, dim=0)\n",
        "    generated_waveform = generated_waveform.view(1, 128, -1)\n",
        "    \n",
        "    mel_inverse = T.InverseMelScale(n_stft=1024, n_mels=128, sample_rate=44100)\n",
        "    griffin_lim = T.GriffinLim(n_fft=2048, n_iter=32)\n",
        "\n",
        "    waveform = griffin_lim(mel_inverse(generated_waveform))\n",
        "    waveform = waveform[:, :int(duration * 44100)]\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "    output_audio_path = os.path.join(output_folder, f\"generated_audio_{timestamp}.wav\")\n",
        "    output_json_path = os.path.join(output_folder, f\"generated_audio_{timestamp}.json\")\n",
        "\n",
        "    torchaudio.save(output_audio_path, waveform, sample_rate=44100)\n",
        "\n",
        "    parameter_names = [\n",
        "        \"Latitude\",\n",
        "        \"Longitude\",\n",
        "        \"Degrees\",\n",
        "        \"Humidity\",\n",
        "        \"Wind speed\",\n",
        "        \"Wind direction\",\n",
        "        \"Pressure\",\n",
        "        \"Elevation\",\n",
        "        \"Minutes of day\",\n",
        "        \"Day of year\",\n",
        "    ]\n",
        "\n",
        "    parameter_data = {name: value for name, value in zip(parameter_names, params)}\n",
        "\n",
        "    with open(output_json_path, \"w\") as f:\n",
        "        json.dump(parameter_data, f, indent=4)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5x4OK5fEZCfA"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "XJ_I83A2Z7VZ",
        "outputId": "61ee561d-c115-4416-e328-d3f8e0d07b34"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Connect to Weights and Biases for tracking progress\n",
        "wandb.init(project=\"audio-gan\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294,
          "referenced_widgets": [
            "cad4f3d85b6b41f587c2e25ae3f876f8",
            "f4042b39b5574ff8bf813942fe747703",
            "5d184e3a850b470f9a4e7b34747c7804",
            "f8c507c116f14c279b89a6bd5118fb77",
            "40c73dc246784395a9461c1792864ff4",
            "9b8a25c42ea6485fa293f3e9765d0024",
            "9e4457a2e93a48eba9b65ceeaee0a245",
            "855d2306bd9d4b13b41e5ccad258ae49"
          ]
        },
        "id": "shzA76G_ZNr4",
        "outputId": "fb69369b-f54c-4c21-d83d-70864bf1fe53"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:wcdwof5f) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.009 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.105588â€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cad4f3d85b6b41f587c2e25ae3f876f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">restful-energy-17</strong> at: <a href='https://wandb.ai/syntheticornithology/audio-gan/runs/wcdwof5f' target=\"_blank\">https://wandb.ai/syntheticornithology/audio-gan/runs/wcdwof5f</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230402_205308-wcdwof5f/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:wcdwof5f). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230402_205347-8owawf6m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/syntheticornithology/audio-gan/runs/8owawf6m' target=\"_blank\">devout-glade-18</a></strong> to <a href='https://wandb.ai/syntheticornithology/audio-gan' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/syntheticornithology/audio-gan' target=\"_blank\">https://wandb.ai/syntheticornithology/audio-gan</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/syntheticornithology/audio-gan/runs/8owawf6m' target=\"_blank\">https://wandb.ai/syntheticornithology/audio-gan/runs/8owawf6m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/syntheticornithology/audio-gan/runs/8owawf6m?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ff8e47f3d00>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_gan(audio_folder, json_folder, epochs=100, batch_size=32, learning_rate=0.0002, device=device, save_interval=50, model_path=model_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GocxAR3CeaxX",
        "outputId": "85d8e508-33ac-4cc8-ed30-654ba316b96f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 593/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2021-06-01-07-30-03-324_P.wavLoading 451/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2020-06-24-10-09-31-180_P.wav\n",
            "\n",
            "Loading 70/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2011-12-28-05-13-38-327_P.wav\n",
            "Loading 390/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2019-12-11-06-17-33-402_P.wav\n",
            "Loading 350/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2019-03-04-09-37-40-809_P.wav\n",
            "Loading 678/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2021-12-17-11-11-31-816_P.wav\n",
            "Loading 465/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2020-06-27-10-09-31-960_P.wav\n",
            "Loading 732/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2022-10-08-07-02-37-367_P.wav\n",
            "Loading 665/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2021-11-23-06-09-28-682_P.wav\n",
            "Loading 94/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2012-01-06-18-26-38-488_P.wav\n",
            "Loading 616/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2021-09-04-08-11-14-427_P.wav\n",
            "Loading 19/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2009-09-08-08-39-52-562_P.wav\n",
            "Loading 499/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2020-08-27-08-10-08-778_P.wav\n",
            "Loading 740/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2022-11-12-07-56-41-168_P.wavLoading 263/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2017-08-26-09-36-24-320_P.wav\n",
            "\n",
            "Loading 744/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2022-12-11-06-41-47-379_P.wav\n",
            "Loading 746/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2022-12-11-08-42-47-729_P.wav\n",
            "Loading 655/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2021-10-30-07-45-32-331_P.wav\n",
            "Loading 37/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2010-10-11-07-40-29-328_P.wav\n",
            "Loading 204/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2016-01-26-06-35-01-566_P.wav\n",
            "Loading 56/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2011-12-22-17-34-37-718_P.wav\n",
            "Loading 31/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2010-10-06-10-10-34-495_P.wav\n",
            "Loading 341/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2019-03-04-01-07-40-762_P.wavLoading 171/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2015-10-17-08-34-47-360_P.wav\n",
            "\n",
            "Loading 642/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2021-10-07-08-18-25-421_P.wav\n",
            "Loading 476/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2020-06-28-08-09-37-228_P.wav\n",
            "Loading 395/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2019-12-19-09-23-27-470_P.wavLoading 299/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2018-08-29-07-07-16-578_P.wav\n",
            "\n",
            "Loading 635/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2021-10-03-08-11-32-676_P.wavLoading 243/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2017-01-12-18-05-51-500_P.wav\n",
            "\n",
            "Loading 343/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2019-03-04-05-37-40-812_P.wavLoading 677/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2021-12-16-06-25-44-452_P.wav\n",
            "\n",
            "Loading 214/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2016-09-04-05-35-43-750_P.wav\n",
            "Loading 144/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2014-10-01-10-03-17-423_P.wav\n",
            "Loading 155/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2015-04-25-07-03-57-184_P.wav\n",
            "Loading 663/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2021-11-20-07-32-28-711_P.wav\n",
            "Loading 368/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2019-10-03-09-08-20-571_P.wav\n",
            "Loading 580/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2021-04-20-06-30-41-038_P.wav\n",
            "Loading 433/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2020-04-17-06-10-07-320_P.wav\n",
            "Loading 516/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2020-08-31-07-09-45-896_P.wav\n",
            "Loading 697/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2022-04-09-18-22-02-701_P.wav\n",
            "Loading 594/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2021-06-12-07-49-04-155_P.wavLoading 477/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2020-06-28-08-09-40-086_P.wav\n",
            "\n",
            "Loading 520/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2020-09-01-07-40-34-961_P.wav\n",
            "Loading 318/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2018-09-05-08-37-20-054_P.wav\n",
            "Loading 195/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2016-01-17-19-15-28-090_P.wav\n",
            "Loading 41/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2011-08-04-10-01-26-012_P.wav\n",
            "Loading 319/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2018-09-05-18-07-20-056_P.wav\n",
            "Loading 715/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2022-08-04-11-12-27-415_P.wav\n",
            "Loading 233/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2016-11-26-10-11-00-372_P.wav\n",
            "Loading 705/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2022-04-21-18-31-05-673_P.wav\n",
            "Loading 213/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2016-09-03-05-51-38-860_P.wav\n",
            "Loading 559/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2020-11-29-10-40-05-917_P.wav\n",
            "Loading 621/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2021-09-19-09-11-15-709_P.wavLoading 704/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2022-04-21-16-37-05-674_P.wav\n",
            "\n",
            "Loading 145/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2014-10-27-05-03-28-148_P.wav\n",
            "Loading 286/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2018-08-22-10-37-14-560_P.wav\n",
            "Loading 295/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2018-08-28-07-07-16-135_P.wav\n",
            "Loading 119/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2013-07-02-14-03-37-897_P.wav\n",
            "Loading 276/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2018-04-24-06-58-01-267_P.wav\n",
            "Loading 346/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2019-03-04-08-37-40-817_P.wav\n",
            "Loading 751/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2022-12-26-06-13-53-060_P.wav\n",
            "Loading 404/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2020-01-15-17-38-46-660_P.wav\n",
            "Loading 131/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2013-11-21-05-32-52-319_P.wav\n",
            "Loading 314/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2018-09-04-08-07-19-621_P.wavLoading 452/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2020-06-25-08-10-09-090_P.wav\n",
            "\n",
            "Loading 249/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2017-04-30-03-36-12-346_P.wav\n",
            "Loading 160/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2015-08-18-06-34-30-510_P.wav\n",
            "Loading 581/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2021-04-22-06-51-41-501_P.wav\n",
            "Loading 408/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2020-01-17-08-08-46-128_P.wavLoading 639/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2021-10-06-08-11-28-696_P.wav\n",
            "\n",
            "Loading 557/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2020-11-25-08-20-21-118_P.wav\n",
            "Loading 62/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2011-12-24-05-27-36-928_P.wav\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-65bdd34cc4e3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-0e2c39e44d91>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(audio_folder, json_folder, epochs, batch_size, learning_rate, device, save_interval, model_path)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpacked_real_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0mpacked_real_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_real_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_real_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-76-0e2c39e44d91>\", line 61, in collate_fn\n    for i, (waveform, _, _) in enumerate(batch):\nValueError: not enough values to unpack (expected 3, got 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 92/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2012-01-05-13-15-36-902_P.wav\n",
            "Loading 331/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2018-11-21-09-10-23-123_P.wav\n",
            "Loading 418/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2020-01-21-06-38-49-054_P.wav\n",
            "Loading 685/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2022-02-12-07-13-47-020_P.wav\n",
            "Loading 731/758: /content/drive/MyDrive/colab_storage/ronxgin_data_samples/2022-10-02-10-57-33-599_P.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example file generation\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  \n",
        "    generator_path = os.path.join(model_path, \"generator_final.pth\")\n",
        "    params = [-24.8874 ,150.9657 , 23.16 , 73 , 4.78 , 8 , 1015 , 506 , 546 , 110]  # Replace with actual parameters\n",
        "    duration = 5.0  # In seconds\n",
        "\n",
        "    generate_audio(generator_path, params, duration, output_path, device)"
      ],
      "metadata": {
        "id": "67CicNbGZfdh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}