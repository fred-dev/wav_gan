{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOf4tpazX+5o2vB78P7+fiF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fred-dev/wav_gan/blob/main/Fred_WAV_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\") # Don't change this.\n",
        "audio_folder = \"/content/drive/MyDrive/colab_storage/ronxgin_data_samples\"\n",
        "json_folder = \"/content/drive/MyDrive/colab_storage/ronxgin_data_samples\"\n",
        "model_path = \"/content/drive/MyDrive/colab_storage/colab_output\"\n",
        "output_path = \"/content/drive/MyDrive/colab_storage/colab_output/\""
      ],
      "metadata": {
        "id": "j-bx2eTsawdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "sNqJUbmNZn5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2fHyssYY8N6"
      },
      "outputs": [],
      "source": [
        "# 1. Import required libraries\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "from torch.utils.data import Dataset, DataLoader, BatchSampler, SubsetRandomSampler\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "from torch.nn.functional import multi_head_attention_forward\n",
        "from torch.nn import MultiheadAttention"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define the dataset class\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, audio_folder, json_folder, transform=None):\n",
        "        # Initialize instance variables for the dataset class\n",
        "        self.audio_folder = audio_folder\n",
        "        self.json_folder = json_folder\n",
        "        self.transform = transform\n",
        "        self.MAX_LENGTH = 400\n",
        "        self.MAX_NUM_FRAMES = 5 * 44100  # 5 seconds of audio frames\n",
        "        self.mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=44100, n_mels=30)\n",
        "        \n",
        "        # Create a sorted list of audio files in the audio_folder\n",
        "        self.file_list = sorted([f for f in os.listdir(audio_folder) if os.path.splitext(f)[1] == '.wav'])\n",
        "\n",
        "    # Define the length method to return the total number of audio files in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "    \n",
        "    # Define the getitem method to return a specific sample from the dataset given its index\n",
        "    def __getitem__(self, idx):\n",
        "        # Construct the path of the audio and JSON files for the current index\n",
        "        audio_path = os.path.join(self.audio_folder, self.file_list[idx])\n",
        "        json_path = os.path.join(self.json_folder, os.path.splitext(self.file_list[idx])[0].rstrip('_P') + \".json\")\n",
        "\n",
        "        print(f\"Loading {idx+1}/{len(self.file_list)}: {audio_path}\")\n",
        "        \n",
        "        # Load the waveform from the audio file and limit the number of frames to MAX_NUM_FRAMES\n",
        "        waveform, _ = torchaudio.load(audio_path, num_frames=self.MAX_NUM_FRAMES)\n",
        "\n",
        "        # Load the JSON file containing parameters\n",
        "        with open(json_path) as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Extract the parameters from the JSON data\n",
        "        params = [\n",
        "            data[\"coord\"][\"lat\"],\n",
        "            data[\"coord\"][\"lon\"],\n",
        "            data[\"wind\"][\"deg\"],\n",
        "            data[\"main\"][\"humidity\"],\n",
        "            data[\"wind\"][\"speed\"],\n",
        "            data[\"wind\"][\"deg\"],\n",
        "            data[\"main\"][\"pressure\"],\n",
        "            data[\"elevation\"],\n",
        "            data[\"minutesOfDay\"],\n",
        "            data[\"dayOfYear\"],\n",
        "        ]\n",
        "\n",
        "        # Apply the Mel spectrogram transformation to the waveform\n",
        "        mel_spec = self.mel_transform(waveform.squeeze())\n",
        "        # Truncate the Mel spectrogram to MAX_LENGTH\n",
        "        mel_spec = mel_spec[:, :self.MAX_LENGTH]\n",
        "\n",
        "        # Convert the parameters to a tensor and expand its dimensions to match the Mel spectrogram\n",
        "        params_tensor = torch.tensor(params, dtype=torch.float32).unsqueeze(1)\n",
        "        params_tensor = params_tensor.expand(-1, mel_spec.size(1))\n",
        "\n",
        "        # Concatenate the Mel spectrogram and the parameters tensor\n",
        "        features = torch.cat((mel_spec, params_tensor), dim=0)\n",
        "\n",
        "        # Return the features tensor and the index as a tensor\n",
        "        return features, torch.tensor(idx, dtype=torch.int64)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Sort the batch by sequence length (descending order) to facilitate padding\n",
        "    batch = sorted(batch, key=lambda x: x[0].size(1), reverse=True)\n",
        "    batch_size = len(batch)  # Determine the batch size from the input batch\n",
        "\n",
        "    # Create a list of the sequence lengths for packed sequences\n",
        "    seq_lengths = [x[0].size(1) for x in batch]\n",
        "\n",
        "    # Initialize a tensor of zeros with the appropriate dimensions for padding\n",
        "    padded_waveforms = torch.zeros(batch_size, batch[0][0].size(0), max(seq_lengths))\n",
        "    for i, (waveform, _) in enumerate(batch):\n",
        "        # Copy the waveform data into the padded_waveforms tensor\n",
        "        padded_waveforms[i, :, :seq_lengths[i]] = waveform\n",
        "\n",
        "    # Convert the padded batch to a packed sequence\n",
        "    # This allows for efficient processing of variable-length sequences\n",
        "    packed_batch = nn.utils.rnn.pack_padded_sequence(padded_waveforms, seq_lengths, batch_first=False, enforce_sorted=True)\n",
        "\n",
        "    # Extract the list of parameter tensors from the batch\n",
        "    params_list = [x[1] for x in batch]\n",
        "    \n",
        "    # Stack the parameter tensors into a single tensor\n",
        "    params_tensor = torch.stack(params_list, dim=0)\n",
        "\n",
        "    # Return the packed batch, the parameters tensor, and a list of indices\n",
        "    return packed_batch, params_tensor\n",
        "\n",
        "\n",
        "# Define the Generator class\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, audio_dim, additional_features=10):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.audio_dim = audio_dim\n",
        "        self.additional_features = additional_features\n",
        "\n",
        "        self.attention = MultiheadAttention(embed_dim=input_dim + additional_features, num_heads=4)\n",
        "        self.lstm = nn.LSTM(input_dim + additional_features, hidden_dim, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, audio_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        output, _ = self.lstm(attn_output)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Define the Discriminator class\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, audio_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.audio_dim = audio_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.attention = MultiheadAttention(embed_dim=audio_dim + 10, num_heads=4)\n",
        "        self.lstm = nn.LSTM(audio_dim + 10, hidden_dim, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, params):\n",
        "        x_with_params = torch.cat((x, params.unsqueeze(1).repeat(1, x.size(1), 1)), dim=2)\n",
        "        attn_output, _ = self.attention(x_with_params, x_with_params, x_with_params)\n",
        "        output, _ = self.lstm(attn_output)\n",
        "        output = self.linear(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_discriminator(real_data, fake_data, params, optimizer, criterion):\n",
        "    # Zero the gradients of the optimizer\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Get predictions from the discriminator for the real data\n",
        "    real_preds = discriminator(real_data, params)\n",
        "\n",
        "    # Calculate the loss for the real data (real_preds should be close to 1)\n",
        "    real_loss = criterion(real_preds, torch.ones_like(real_preds))\n",
        "\n",
        "    # Get predictions from the discriminator for the fake (generated) data\n",
        "    fake_preds = discriminator(fake_data, params)\n",
        "\n",
        "    # Calculate the loss for the fake data (fake_preds should be close to 0)\n",
        "    fake_loss = criterion(fake_preds, torch.zeros_like(fake_preds))\n",
        "\n",
        "    # Calculate the total loss by summing the real and fake losses\n",
        "    total_loss = real_loss + fake_loss\n",
        "    \n",
        "    # Perform backpropagation to update the discriminator's weights\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return total_loss.item()\n",
        "\n",
        "def train_generator(fake_data, params, optimizer, criterion):\n",
        "    # Zero the gradients of the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Get predictions from the discriminator for the fake (generated) data\n",
        "    preds = discriminator(fake_data, params)\n",
        "\n",
        "    # Calculate the loss for the generator (preds should be close to 1)\n",
        "    loss = criterion(preds, torch.ones_like(preds))\n",
        "\n",
        "    # Perform backpropagation to update the generator's weights\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def train_gan(audio_folder, json_folder, epochs, batch_size, learning_rate, device, save_interval, model_path):\n",
        "    MAX_LENGTH = 431 \n",
        "\n",
        "    # Define the MEL spectrogram transformation\n",
        "    mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(sample_rate=44100, n_mels=128, hop_length=1024, n_fft=2048)\n",
        "    mel_spectrogram_transform.n_mels = 30 # Set the number of Mel bands\n",
        "\n",
        "    # Create the dataset and dataloader \n",
        "    dataset = AudioDataset(audio_folder, json_folder, transform=mel_spectrogram_transform)\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_sampler=BatchSampler(SubsetRandomSampler(range(len(dataset))), batch_size, drop_last=True), collate_fn=collate_fn)\n",
        "\n",
        "    \n",
        "    input_dim = 30\n",
        "    hidden_dim = 128\n",
        "    num_layers = 1\n",
        "    audio_dim = 30  #Set the audio dimension 30 Mel bands\n",
        "    \n",
        "    # Initialize the generator and discriminator models\n",
        "    generator = Generator(input_dim, hidden_dim, num_layers, audio_dim).to(device)\n",
        "    discriminator = Discriminator(audio_dim, hidden_dim, num_layers, 1).to(device)\n",
        "\n",
        "    # Save the initial state of the generator and discriminator models, if they dont exist already\n",
        "    initial_generator_path = os.path.join(model_path, \"generator_initial.pth\")\n",
        "    initial_discriminator_path = os.path.join(model_path, \"discriminator_initial.pth\")\n",
        "\n",
        "    if not os.path.exists(initial_generator_path):\n",
        "        torch.save(generator.state_dict(), initial_generator_path)\n",
        "\n",
        "    if not os.path.exists(initial_discriminator_path):\n",
        "        torch.save(discriminator.state_dict(), initial_discriminator_path)\n",
        "\n",
        "    # Set up the loss function and optimizers for generator and discriminator\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
        "    optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
        "\n",
        "    # Train the GAN for the specified N epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        for batch_idx, (packed_real_data, params) in enumerate(dataloader):\n",
        "            packed_real_data, params = packed_real_data.to(device), params.to(device)\n",
        "            batch_size = packed_real_data.batch_sizes[0]\n",
        "\n",
        "\n",
        "            # Train discriminator\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            noise = torch.randn(batch_size, input_dim, device=device)\n",
        "            z = noise.unsqueeze(1) + params.unsqueeze(1).unsqueeze(2)\n",
        "            packed_fake_data = generator(z)\n",
        "\n",
        "            real_data, _ = pad_packed_sequence(packed_real_data, batch_first=True)\n",
        "            fake_data, _ = pad_packed_sequence(packed_fake_data, batch_first=True)\n",
        "\n",
        "            # Calculate the losses for real and fake data and update the discriminator's weights\n",
        "            real_validity = discriminator(torch.cat((real_data, params.unsqueeze(1).repeat(1, real_data.size(1), 1)), dim=2))\n",
        "            fake_validity = discriminator(torch.cat((fake_data.detach(), params.unsqueeze(1).repeat(1, fake_data.size(1), 1)), dim=2))\n",
        "\n",
        "            real_loss = criterion(real_validity, torch.ones(batch_size, 1, device=device))\n",
        "            fake_loss = criterion(fake_validity, torch.zeros(batch_size, 1, device=device))\n",
        "            d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # Train generator\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            # Calculate the generator's loss and update its weights\n",
        "            fake_validity = discriminator(torch.cat((fake_data, params.unsqueeze(1).repeat(1, fake_data.size(1), 1)), dim=2))\n",
        "            g_loss = criterion(fake_validity, torch.ones(batch_size, 1, device=device))\n",
        "\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            # Print the losses for the current batch\n",
        "            print(f\"Epoch [{epoch}/{epochs}] Batch [{batch_idx+1}/{len(dataloader)}] Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}\")\n",
        "        \n",
        "         # Save the generator and discriminator models \n",
        "        if epoch % save_interval == 0:\n",
        "            torch.save(generator.state_dict(), os.path.join(model_path, f\"generator_epoch_{epoch}.pth\"))\n",
        "            torch.save(discriminator.state_dict(), os.path.join(model_path, f\"discriminator_epoch_{epoch}.pth\"))\n",
        "\n",
        "    # Save the final generator and discriminator models after training\n",
        "    torch.save(generator.state_dict(), os.path.join(model_path, \"generator_final.pth\"))\n",
        "    torch.save(discriminator.state_dict(), os.path.join(model_path, \"discriminator_final.pth\"))\n",
        "\n",
        "\n",
        "\n",
        "def generate_audio(generator_path, params, duration, output_folder, device):\n",
        "    # Load the generator model\n",
        "    generator = Generator(input_dim=40, hidden_dim=128, num_layers=1, audio_dim=30).to(device)\n",
        "    generator.load_state_dict(torch.load(generator_path))\n",
        "    generator.eval()\n",
        "    \n",
        "    # Convert input parameters to a tensor and move it to the device\n",
        "    params = torch.tensor(params, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    \n",
        "    # Calculate the number of steps needed for the given duration\n",
        "    num_steps = int(duration * 44100 / 1024)\n",
        "    \n",
        "    # Initialize an empty list to store generated waveforms\n",
        "    generated_waveforms = []\n",
        "\n",
        "    print(\"Generating audio...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Generate random noise\n",
        "        for step in range(num_steps):\n",
        "            # Generate random noise\n",
        "            noise = torch.randn(1, 1, 128 * 128 - 10, device=device)\n",
        "\n",
        "            # Concatenate the noise and parameters\n",
        "            z = torch.cat((noise, params.unsqueeze(1)), dim=2)\n",
        "\n",
        "            # Generate the output waveform using the generator\n",
        "            output, _ = generator(z)\n",
        "            output_waveform = output.squeeze().detach().cpu()\n",
        "\n",
        "            # Append the generated waveform to the list\n",
        "            generated_waveforms.append(output_waveform)\n",
        "\n",
        "            if step % (num_steps // 10) == 0:\n",
        "                print(f\"Step {step}/{num_steps}\")\n",
        "\n",
        "    print(\"Audio generation completed.\")\n",
        "\n",
        "    # Concatenate generated waveforms into a single tensor\n",
        "    generated_waveform = torch.cat(generated_waveforms, dim=0)\n",
        "    generated_waveform = generated_waveform.view(1, 128, -1)\n",
        "    \n",
        "    # Initialize the Inverse Mel Scale and Griffin-Lim transforms\n",
        "    mel_inverse = T.InverseMelScale(n_stft=1024, n_mels=128, sample_rate=44100)\n",
        "    griffin_lim = T.GriffinLim(n_fft=2048, n_iter=32)\n",
        "\n",
        "    # Convert the generated Mel spectrogram back to a waveform\n",
        "    waveform = griffin_lim(mel_inverse(generated_waveform))\n",
        "    waveform = waveform[:, :int(duration * 44100)]\n",
        "\n",
        "    # Save the generated audio and parameters\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "    output_audio_path = os.path.join(output_folder, f\"generated_audio_{timestamp}.wav\")\n",
        "    output_json_path = os.path.join(output_folder, f\"generated_audio_{timestamp}.json\")\n",
        "\n",
        "    torchaudio.save(output_audio_path, waveform, sample_rate=44100)\n",
        "\n",
        "    parameter_names = [\n",
        "        \"Latitude\",\n",
        "        \"Longitude\",\n",
        "        \"Degrees\",\n",
        "        \"Humidity\",\n",
        "        \"Wind speed\",\n",
        "        \"Wind direction\",\n",
        "        \"Pressure\",\n",
        "        \"Elevation\",\n",
        "        \"Minutes of day\",\n",
        "        \"Day of year\",\n",
        "    ]\n",
        "\n",
        "    # Save the input parameters as a JSON file  \n",
        "    parameter_data = {name: value for name, value in zip(parameter_names, params)}\n",
        "\n",
        "    with open(output_json_path, \"w\") as f:\n",
        "        json.dump(parameter_data, f, indent=4)"
      ],
      "metadata": {
        "id": "5x4OK5fEZCfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "XJ_I83A2Z7VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Connect to Weights and Biases for tracking progress\n",
        "wandb.init(project=\"audio-gan\")"
      ],
      "metadata": {
        "id": "shzA76G_ZNr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_gan(audio_folder, json_folder, epochs=100, batch_size=40, learning_rate=0.0002, device=device, save_interval=50, model_path=model_path)"
      ],
      "metadata": {
        "id": "GocxAR3CeaxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example file generation\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  \n",
        "    generator_path = os.path.join(model_path, \"generator_final.pth\")\n",
        "    params = [-24.8874 ,150.9657 , 23.16 , 73 , 4.78 , 8 , 1015 , 506 , 546 , 110]  # Replace with actual parameters\n",
        "    duration = 5.0  # In seconds\n",
        "\n",
        "    generate_audio(generator_path, params, duration, output_path, device)"
      ],
      "metadata": {
        "id": "67CicNbGZfdh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}